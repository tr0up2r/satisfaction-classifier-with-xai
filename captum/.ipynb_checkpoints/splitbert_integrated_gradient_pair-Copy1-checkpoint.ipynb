{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8991db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.lang.en import English\n",
    "from splitbert.textsplit import text_segmentation\n",
    "from splitbert.SplitBertEncoderAttentionModel import SplitBertEncoderAttentionModel\n",
    "from splitbert.utils import conduct_input_ids_and_attention_masks\n",
    "from splitbert.utils import make_masks\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1668f",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a53f02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seg', 'seg', 'snt']\n",
      "10 4 10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "post_df = pd.read_csv('../predicting-satisfaction-using-graphs/csv/dataset/liwc_post.csv', encoding='UTF-8')\n",
    "comment_df = pd.read_csv('../predicting-satisfaction-using-graphs/csv/dataset/liwc_comment.csv', encoding='UTF-8')\n",
    "reply_df = pd.read_csv('../predicting-satisfaction-using-graphs/csv/dataset/avg_satisfaction_raw_0-999.csv', encoding='ISO-8859-1')\n",
    "\n",
    "modes = [['seg', 'seg', 'snt']]\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# satisfaction score (y)\n",
    "satisfactions_float = list(reply_df['satisfy_composite'])\n",
    "satisfactions = []\n",
    "\n",
    "for s in satisfactions_float:\n",
    "    if s < 3.5:\n",
    "        satisfactions.append(0)\n",
    "    elif s < 5:\n",
    "        satisfactions.append(1)\n",
    "    else:\n",
    "        satisfactions.append(2)\n",
    "\n",
    "reply_contents = list(reply_df['replyContent'])\n",
    "post_contents = list(post_df['content'])\n",
    "comment_bodies = list(comment_df['content'])\n",
    "\n",
    "\n",
    "def get_sequences(contents, mode):\n",
    "    sequences = []\n",
    "\n",
    "    if mode == 'all':\n",
    "        for content in contents:\n",
    "            sequences.append([content])\n",
    "    elif mode == 'seg':\n",
    "        for content in contents:\n",
    "            sentences = list(map(lambda x: str(x), list(nlp(content).sents)))\n",
    "            sequences.append(text_segmentation(sentences))\n",
    "    else:  # sentences\n",
    "        for content in contents:\n",
    "            sequences.append(list(map(lambda x: str(x), list(nlp(content).sents))))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "for mode in modes:\n",
    "    print(mode)\n",
    "    post_sequences = get_sequences(post_contents, mode[0])\n",
    "    comment_sequences = get_sequences(comment_bodies, mode[1])\n",
    "    reply_sequences = get_sequences(reply_contents, mode[2])\n",
    "\n",
    "    data = []\n",
    "    max_post, max_comment, max_reply = 0, 0, 0\n",
    "    i = 0\n",
    "    for post, comment, reply, satisfaction, satisfaction_float in zip(post_sequences, comment_sequences,\n",
    "                                                                          reply_sequences, satisfactions,\n",
    "                                                                          satisfactions_float):\n",
    "        if len(post) > max_post:\n",
    "            max_post = len(post)\n",
    "        if len(comment) > max_comment:\n",
    "            max_comment = len(comment)\n",
    "        if len(reply) > max_reply:\n",
    "            max_reply = len(reply)\n",
    "\n",
    "        data.append([i, post, comment, reply, satisfaction, satisfaction_float])\n",
    "        i += 1\n",
    "\n",
    "    print(max_post, max_comment, max_reply)\n",
    "    max_count = max(max_post, max_comment, max_reply)\n",
    "    print(max_count)\n",
    "\n",
    "    columns = ['index', 'post_contents', 'comment_contents', 'reply_contents', 'label', 'score']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # data split (train & test sets)\n",
    "    idx_train, idx_remain = train_test_split(df.index.values, test_size=0.20, random_state=42)\n",
    "    idx_val, idx_test = train_test_split(idx_remain, test_size=0.50, random_state=42)\n",
    "\n",
    "    train_df = df.iloc[idx_train]\n",
    "    val_df = df.iloc[idx_val]\n",
    "    test_df = df.iloc[idx_test]\n",
    "\n",
    "    count_min_label = min(train_df['label'].value_counts())\n",
    "\n",
    "    labels = [0, 1, 2]\n",
    "\n",
    "    train_sample_df = pd.DataFrame([], columns=columns)\n",
    "\n",
    "    for label in labels:\n",
    "        tmp = train_df[train_df['label'] == label]\n",
    "        tmp_sampled = tmp.sample(frac=1).iloc[:count_min_label]\n",
    "        train_sample_df = pd.concat([train_sample_df, tmp_sampled])\n",
    "\n",
    "    train_sample_df = train_sample_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0bf40",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637e711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(data):\n",
    "    return (data - torch.min(data)) / (torch.max(data) - torch.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e805e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func_ig(inputs, p_count, c_count, model):\n",
    "    batch_embeddings = inputs[0]\n",
    "    encoder_outputs = torch.empty(size=(1, model.embedding_size * 2), requires_grad=True).to(model.device)\n",
    "    outputs_list = []\n",
    "\n",
    "    for i in range(len(batch_embeddings)):\n",
    "        non_zero_rows = batch_embeddings[i][0][batch_embeddings[i][0].sum(dim=1) != 0]\n",
    "        zero_rows = torch.zeros((batch_embeddings[i][0].shape[0] - non_zero_rows.shape[0], model.embedding_size),\n",
    "                                    dtype=torch.int, device=model.device)\n",
    "        batch_embeddings[i] = torch.cat([non_zero_rows, zero_rows])\n",
    "\n",
    "        i = 0\n",
    "    for embeddings, p_count, c_count in zip(batch_embeddings, sentence_count_list[0], sentence_count_list[1]):\n",
    "        embeddings = embeddings.swapaxes(0, 1)\n",
    "        outputs_list.append(embeddings)\n",
    "        src_mask, src_key_padding_mask = make_masks(model.max_post_len, [p_count, c_count], model.device,\n",
    "                                                        model.max_post_len, model.max_comment_len,\n",
    "                                                        \"concat_all\")\n",
    "        if model.encoder_mask_mode:\n",
    "            encoder_output = model.encoder(embeddings, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        else:\n",
    "            encoder_output = model.encoder(embeddings, src_key_padding_mask=src_key_padding_mask)\n",
    "        encoder_outputs[i][:model.embedding_size] = torch.mean(encoder_output[:p_count+c_count], dim=0).squeeze(0)\n",
    "\n",
    "        if model.attention_mask_mode:\n",
    "            attention = model.mhead_attention(encoder_output, encoder_output, encoder_output, attn_mask=src_mask,\n",
    "                                                 key_padding_mask=src_key_padding_mask)[0]\n",
    "        else:\n",
    "            attention = model.mhead_attention(encoder_output, encoder_output, encoder_output, key_padding_mask=src_key_padding_mask)[0]\n",
    "\n",
    "        # mul mask - diagonal masking\n",
    "        attention = attention.swapaxes(0, 2)\n",
    "        mask = torch.tensor([1] * (p_count + c_count) + [0] * (model.max_post_len + model.max_comment_len - (p_count + c_count))).to(\n",
    "                model.device)\n",
    "        attention = attention.mul(mask).swapaxes(0, 2)\n",
    "\n",
    "        attention = torch.flatten(attention)\n",
    "        attention = model.attn_classifier1(attention)\n",
    "        attention = model.attn_classifier2(attention)\n",
    "        encoder_outputs[i][model.embedding_size:] = attention\n",
    "        i += 1\n",
    "    encoder_outputs = model.mean_attn_layer(encoder_outputs)\n",
    "    logits = model.classifier2(encoder_outputs)\n",
    "    if model.softmax:\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "\n",
    "\n",
    "ig = IntegratedGradients(forward_func_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10ede48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(target, path, epoch, encoder_mask_mode, attention_mask_mode, softmax):\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    model_path = f'{path}/epoch_{epoch}.model'\n",
    "    \n",
    "    model = SplitBertEncoderAttentionModel(num_labels=len(labels), embedding_size=384, max_len=max_count,\n",
    "                                               max_post_len=max_post, max_comment_len=max_comment, device=device,\n",
    "                                               target=\"post_comment\", encoder_mask_mode=encoder_mask_mode,\n",
    "                                               attention_mask_mode=attention_mask_mode, softmax=softmax)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "\n",
    "    for param in model.sbert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return device, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f529f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(targets):\n",
    "    input_ids_list, ref_input_ids_list, attention_masks_list, sentence_count_list = [], [], [], []\n",
    "    \n",
    "    for contents in targets:\n",
    "        result = tokenizer(contents, pad_to_max_length=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "        \n",
    "        input_ids = result['input_ids']\n",
    "        sentence_count_list.append(torch.tensor(len(input_ids)).unsqueeze(0))\n",
    "        attention_masks = result['attention_mask']\n",
    "        \n",
    "        pad = (0, 0, 0, max_count-len(input_ids))\n",
    "        input_ids = nn.functional.pad(input_ids, pad, \"constant\", 0)\n",
    "        attention_masks = nn.functional.pad(attention_masks, pad, \"constant\", 0)\n",
    "        ref_input_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        input_ids_list.append(input_ids.unsqueeze(0))\n",
    "        ref_input_ids_list.append(ref_input_ids.unsqueeze(0))\n",
    "        attention_masks_list.append(attention_masks.unsqueeze(0))\n",
    "    \n",
    "    return input_ids_list, ref_input_ids_list, attention_masks_list, sentence_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f2e6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_attributions(attribution):\n",
    "    attributions = attribution.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d435e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexes(filename):\n",
    "    index_df = pd.read_csv(filename, encoding='UTF-8')\n",
    "    index_df.columns = ['Unnamed: 0', 'prediction', 'label', 'score', 'idx']\n",
    "    val_index = sorted(list(index_df.idx.values))\n",
    "    \n",
    "    return val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7735b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = get_indexes(f'../predicting-satisfaction-using-graphs/csv/splitbert_classifier/post_comment/seg_seg/epoch_4_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39c6191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitbert_integrated_gradient_post_comment(index, post, comment, p_sentences, c_sentences, label, score, visualize=False, softmax=False):\n",
    "    \n",
    "    def post_or_comment_or_reply(index):\n",
    "        for i, sentences in enumerate(all_sentences):\n",
    "            if all_tokens[index] in sentences:\n",
    "                if i == 0:\n",
    "                    return 'post'\n",
    "                elif i == 1:\n",
    "                    return 'comment'\n",
    "                else:\n",
    "                    return 'reply'\n",
    "    \n",
    "    input_ids, ref_input_ids, attention_masks, sentence_counts = construct_input_ref_pair([post, comment])\n",
    "    \n",
    "    one_hot_labels = torch.nn.functional.one_hot(torch.tensor(label), num_classes=len(labels))\n",
    "    inputs = {'labels': one_hot_labels.type(torch.float).to(device),\n",
    "          'input_ids1': input_ids[0].to(device),\n",
    "          'input_ids2': input_ids[1].to(device),\n",
    "          'attention_mask1': attention_masks[0].to(device),\n",
    "          'attention_mask2': attention_masks[1].to(device),\n",
    "          'sentence_count1': sentence_counts[0].to(device),\n",
    "          'sentence_count2': sentence_counts[1].to(device)\n",
    "         }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = pc_model(**inputs).hidden_states\n",
    "\n",
    "    # inputs = torch.stack(embeddings, dim=0)\n",
    "    baselines = torch.zeros((1, 1, 14, 384))\n",
    "    pred = forward_func_ig(inputs, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "    base_pred = forward_func_ig(baselines, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "        \n",
    "    print(f'pred: {pred}, base_pred: {base_pred}')\n",
    "    print(f'sub: {pred - base_pred}')\n",
    "    print('=======================================================')\n",
    "    \n",
    "    pred = forward_func_ig(inputs, sentence_counts[0], sentence_counts[1], pc_model, False)\n",
    "    base_pred = forward_func_ig(baselines, sentence_counts[0], sentence_counts[1], pc_model, False)\n",
    "        \n",
    "    print(f'pred: {pred}, base_pred: {base_pred}')\n",
    "    print(f'sub: {pred - base_pred}')\n",
    "    \n",
    "#     pred = forward_func_ig(inputs, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "#     base_pred = forward_func_ig(baselines, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "#     print(f'pred: {pred}, base_pred: {base_pred}')\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    \n",
    "    attribution, delta = ig.attribute(inputs=inputs, target=torch.argmax(pred), additional_forward_args=(sentence_counts[0], sentence_counts[1], pc_model), n_steps=1, return_convergence_delta=True)\n",
    "    attributions = summarize_attributions(attribution)\n",
    "    f_attributions = torch.flatten(attributions)\n",
    "    f_attributions = f_attributions[f_attributions.nonzero()].squeeze(1)\n",
    "    abs_attributions = list(map(abs, map(float, f_attributions)))\n",
    "    idx_attributions = []\n",
    "    for j in range(len(abs_attributions)):\n",
    "        idx_attributions.append((j, abs_attributions[j], f_attributions[j].item()))\n",
    "    idx_attributions.sort(key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "    top3 = idx_attributions[:3]\n",
    "    \n",
    "    if visualize:\n",
    "        all_sentences = [['[[post]]'], post, ['[[comment]]'], comment]\n",
    "        all_tokens = [item for all_sentences in all_sentences for item in all_sentences]\n",
    "        \n",
    "        vis_attributions = []\n",
    "        \n",
    "        j = 0\n",
    "        for i in range(len(all_tokens)):\n",
    "            if all_tokens[i] in ['[[post]]', '[[comment]]']:\n",
    "                vis_attributions.append(0)\n",
    "            else:\n",
    "                vis_attributions.append(f_attributions[j].item())\n",
    "                j += 1\n",
    "                \n",
    "        vis_attributions = torch.tensor(vis_attributions)\n",
    "        \n",
    "        score_vis = viz.VisualizationDataRecord(vis_attributions,\n",
    "                                                torch.max(torch.softmax(pred, dim=0)),\n",
    "                                                torch.argmax(pred),  # predicted label\n",
    "                                                f'{label}, {score}',  # true label\n",
    "                                                p_sentences + ' ' + c_sentences,\n",
    "                                                vis_attributions.sum(),\n",
    "                                                all_tokens,\n",
    "                                                delta)\n",
    "        raw_text = ' '.join(post) + ' '.join(comment)\n",
    "        \n",
    "        print('\\033[1m', 'Visualization For Score', '\\033[0m')\n",
    "        viz.visualize_text([score_vis])\n",
    "        print(vis_attributions)\n",
    "        \n",
    "    else:\n",
    "        where = []\n",
    "        \n",
    "        all_sentences = [post, comment]\n",
    "        all_tokens = [item for all_sentences in all_sentences for item in all_sentences]\n",
    "\n",
    "        for j in range(len(top3)):\n",
    "            where.append(post_or_comment_or_reply(top3[j][0]))\n",
    "            result.append([index, post, comment, score, all_tokens[top3[j][0]], top3[j][2], post_or_comment_or_reply(top3[j][0])])\n",
    "\n",
    "        # result.append([index, post, comment, score, all_tokens[top3[0][0]], top3[0][2], where[0]])\n",
    "        # result.append([index, post, comment, score, all_tokens[top3[1][0]], top3[1][2], where[1]])\n",
    "        # result.append([index, post, comment, score, all_tokens[top3[2][0]], top3[2][2], where[2]])\n",
    "\n",
    "        return result, label, torch.argmax(pred).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a64a29d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device, pc_model, tokenizer = prepare_model('post_comment', '../predicting-satisfaction-using-graphs/splitbert/model/seg_seg/attention', 5, False, 'diagonal', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "691b0f01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1460046/3826429340.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msplitbert_integrated_gradient_post_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_bodies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msatisfactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msatisfactions_float\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1460046/417070203.py\u001b[0m in \u001b[0;36msplitbert_integrated_gradient_post_comment\u001b[0;34m(index, post, comment, p_sentences, c_sentences, label, score, visualize, softmax)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# inputs = torch.stack(embeddings, dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbaselines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_func_ig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mbase_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_func_ig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1460046/1061129399.py\u001b[0m in \u001b[0;36mforward_func_ig\u001b[0;34m(inputs, p_count, c_count, model)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mnon_zero_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         zero_rows = torch.zeros((batch_embeddings[i][0].shape[0] - non_zero_rows.shape[0], model.embedding_size),\n\u001b[1;32m      9\u001b[0m                                     dtype=torch.int, device=model.device)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "for i in index_list[:10]:\n",
    "    splitbert_integrated_gradient_post_comment(i, post_sequences[i], comment_sequences[i], post_contents[i], comment_bodies[i], satisfactions[i], satisfactions_float[i], True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ebfe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ffd910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213258d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83bbc882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device, pc_model, tokenizer = prepare_model('post_comment', '../predicting-satisfaction-using-graphs/splitbert/model/seg_seg/wo_softmax', 4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d6a4b67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward_func_ig() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1460046/2751330555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msplitbert_integrated_gradient_post_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_bodies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msatisfactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msatisfactions_float\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1460046/2938923834.py\u001b[0m in \u001b[0;36msplitbert_integrated_gradient_post_comment\u001b[0;34m(index, post, comment, p_sentences, c_sentences, label, score, visualize, softmax)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# inputs = torch.stack(embeddings, dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbaselines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_func_ig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mbase_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_func_ig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward_func_ig() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "for i in index_list[:1]:\n",
    "    splitbert_integrated_gradient_post_comment(i, post_sequences[i], comment_sequences[i], post_contents[i], comment_bodies[i], satisfactions[i], satisfactions_float[i], True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc6719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
