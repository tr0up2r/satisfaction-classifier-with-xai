{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8991db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import IntegratedGradients\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.lang.en import English\n",
    "from splitbert.textsplit import text_segmentation\n",
    "from splitbert.SplitBertEncoderAttentionModel import SplitBertEncoderAttentionModel\n",
    "from splitbert.utils import conduct_input_ids_and_attention_masks\n",
    "from splitbert.utils import make_masks\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1668f",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a53f02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['seg', 'seg', 'snt']\n",
      "10 4 10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "post_df = pd.read_csv('../predicting-satisfaction-using-graphs/csv/dataset/liwc_post.csv', encoding='UTF-8')\n",
    "comment_df = pd.read_csv('../predicting-satisfaction-using-graphs/csv/dataset/liwc_comment.csv', encoding='UTF-8')\n",
    "reply_df = pd.read_csv('../predicting-satisfaction-using-graphs/csv/dataset/avg_satisfaction_raw_0-999.csv', encoding='ISO-8859-1')\n",
    "\n",
    "modes = [['seg', 'seg', 'snt']]\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "# satisfaction score (y)\n",
    "satisfactions_float = list(reply_df['satisfy_composite'])\n",
    "satisfactions = []\n",
    "\n",
    "for s in satisfactions_float:\n",
    "    if s < 3.5:\n",
    "        satisfactions.append(0)\n",
    "    elif s < 5:\n",
    "        satisfactions.append(1)\n",
    "    else:\n",
    "        satisfactions.append(2)\n",
    "\n",
    "reply_contents = list(reply_df['replyContent'])\n",
    "post_contents = list(post_df['content'])\n",
    "comment_bodies = list(comment_df['content'])\n",
    "\n",
    "\n",
    "def get_sequences(contents, mode):\n",
    "    sequences = []\n",
    "\n",
    "    if mode == 'all':\n",
    "        for content in contents:\n",
    "            sequences.append([content])\n",
    "    elif mode == 'seg':\n",
    "        for content in contents:\n",
    "            sentences = list(map(lambda x: str(x), list(nlp(content).sents)))\n",
    "            sequences.append(text_segmentation(sentences))\n",
    "    else:  # sentences\n",
    "        for content in contents:\n",
    "            sequences.append(list(map(lambda x: str(x), list(nlp(content).sents))))\n",
    "\n",
    "    return sequences\n",
    "\n",
    "\n",
    "for mode in modes:\n",
    "    print(mode)\n",
    "    post_sequences = get_sequences(post_contents, mode[0])\n",
    "    comment_sequences = get_sequences(comment_bodies, mode[1])\n",
    "    reply_sequences = get_sequences(reply_contents, mode[2])\n",
    "\n",
    "    data = []\n",
    "    max_post, max_comment, max_reply = 0, 0, 0\n",
    "    i = 0\n",
    "    for post, comment, reply, satisfaction, satisfaction_float in zip(post_sequences, comment_sequences,\n",
    "                                                                          reply_sequences, satisfactions,\n",
    "                                                                          satisfactions_float):\n",
    "        if len(post) > max_post:\n",
    "            max_post = len(post)\n",
    "        if len(comment) > max_comment:\n",
    "            max_comment = len(comment)\n",
    "        if len(reply) > max_reply:\n",
    "            max_reply = len(reply)\n",
    "\n",
    "        data.append([i, post, comment, reply, satisfaction, satisfaction_float])\n",
    "        i += 1\n",
    "\n",
    "    print(max_post, max_comment, max_reply)\n",
    "    max_count = max(max_post, max_comment, max_reply)\n",
    "    print(max_count)\n",
    "\n",
    "    columns = ['index', 'post_contents', 'comment_contents', 'reply_contents', 'label', 'score']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "    # data split (train & test sets)\n",
    "    idx_train, idx_remain = train_test_split(df.index.values, test_size=0.20, random_state=42)\n",
    "    idx_val, idx_test = train_test_split(idx_remain, test_size=0.50, random_state=42)\n",
    "\n",
    "    train_df = df.iloc[idx_train]\n",
    "    val_df = df.iloc[idx_val]\n",
    "    test_df = df.iloc[idx_test]\n",
    "\n",
    "    count_min_label = min(train_df['label'].value_counts())\n",
    "\n",
    "    labels = [0, 1, 2]\n",
    "\n",
    "    train_sample_df = pd.DataFrame([], columns=columns)\n",
    "\n",
    "    for label in labels:\n",
    "        tmp = train_df[train_df['label'] == label]\n",
    "        tmp_sampled = tmp.sample(frac=1).iloc[:count_min_label]\n",
    "        train_sample_df = pd.concat([train_sample_df, tmp_sampled])\n",
    "\n",
    "    train_sample_df = train_sample_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e0bf40",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "637e711e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(data):\n",
    "    return (data - torch.min(data)) / (torch.max(data) - torch.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "5e805e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func_ig(inputs, p_count, c_count, model):\n",
    "    embeddings = inputs\n",
    "    encoder_outputs = torch.empty(size=(1, model.embedding_size * 2)).to(model.device)\n",
    "    outputs_list = []\n",
    "\n",
    "    non_zero_rows = embeddings[0][embeddings[0].sum(dim=1) != 0]\n",
    "    zero_rows = torch.zeros((embeddings[0].shape[0] - non_zero_rows.shape[0], model.embedding_size),\n",
    "                            dtype=torch.int, device=model.device)\n",
    "    embeddings = torch.cat([non_zero_rows, zero_rows])\n",
    "    embeddings = embeddings.unsqueeze(0)\n",
    "    embeddings = embeddings.swapaxes(0, 1)\n",
    "    outputs_list.append(embeddings)\n",
    "    src_mask, src_key_padding_mask = make_masks(model.max_post_len, [p_count, c_count], model.device, model.max_post_len, \n",
    "                                                model.max_comment_len, \"concat_all\")\n",
    "    if model.encoder_mask_mode:\n",
    "        encoder_output = model.encoder(embeddings, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "    else:\n",
    "        encoder_output = model.encoder(embeddings, src_key_padding_mask=src_key_padding_mask)\n",
    "    encoder_outputs[0][:model.embedding_size] = torch.mean(encoder_output[:p_count+c_count], dim=0).squeeze(0)\n",
    "\n",
    "    if model.attention_mask_mode:\n",
    "        attention = model.mhead_attention(encoder_output, encoder_output, encoder_output, attn_mask=src_mask,\n",
    "                                          key_padding_mask=src_key_padding_mask)[0]\n",
    "    else:\n",
    "        attention = model.mhead_attention(encoder_output, encoder_output, encoder_output, key_padding_mask=src_key_padding_mask)[0]\n",
    "\n",
    "    # mul mask - diagonal masking\n",
    "    attention = attention.swapaxes(0, 2)\n",
    "    mask = torch.tensor([1] * (p_count + c_count) + [0] * (model.max_post_len + model.max_comment_len - (p_count + c_count))).to(\n",
    "            model.device)\n",
    "    attention = attention.mul(mask).swapaxes(0, 2)\n",
    "\n",
    "    attention = torch.flatten(attention)\n",
    "    attention = model.attn_classifier1(attention)\n",
    "    attention = model.attn_classifier2(attention)\n",
    "    encoder_outputs[0][model.embedding_size:] = attention\n",
    "    \n",
    "    encoder_outputs = model.mean_attn_layer(encoder_outputs)\n",
    "    logits = model.classifier2(encoder_outputs)\n",
    "    if model.softmax:\n",
    "        logits = F.softmax(logits, dim=1)\n",
    "    return logits\n",
    "    \n",
    "\n",
    "ig = IntegratedGradients(forward_func_ig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "10ede48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(target, path, epoch, encoder_mask_mode, attention_mask_mode, softmax):\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    \n",
    "    model_path = f'{path}/epoch_{epoch}.model'\n",
    "    \n",
    "    model = SplitBertEncoderAttentionModel(num_labels=len(labels), embedding_size=384, max_len=max_count,\n",
    "                                               max_post_len=max_post, max_comment_len=max_comment, device=device,\n",
    "                                               target=\"post_comment\", encoder_mask_mode=encoder_mask_mode,\n",
    "                                               attention_mask_mode=attention_mask_mode, softmax=softmax)\n",
    "\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.to('cpu')\n",
    "    model.eval()\n",
    "\n",
    "    for param in model.sbert.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for param in model.bert.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return device, model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f529f25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(targets):\n",
    "    input_ids_list, ref_input_ids_list, attention_masks_list, sentence_count_list = [], [], [], []\n",
    "    \n",
    "    for contents in targets:\n",
    "        result = tokenizer(contents, pad_to_max_length=True, truncation=True, max_length=256, return_tensors='pt')\n",
    "        \n",
    "        input_ids = result['input_ids']\n",
    "        sentence_count_list.append(torch.tensor(len(input_ids)).unsqueeze(0))\n",
    "        attention_masks = result['attention_mask']\n",
    "        \n",
    "        pad = (0, 0, 0, max_count-len(input_ids))\n",
    "        input_ids = nn.functional.pad(input_ids, pad, \"constant\", 0)\n",
    "        attention_masks = nn.functional.pad(attention_masks, pad, \"constant\", 0)\n",
    "        ref_input_ids = torch.zeros_like(input_ids)\n",
    "\n",
    "        input_ids_list.append(input_ids.unsqueeze(0))\n",
    "        ref_input_ids_list.append(ref_input_ids.unsqueeze(0))\n",
    "        attention_masks_list.append(attention_masks.unsqueeze(0))\n",
    "    \n",
    "    return input_ids_list, ref_input_ids_list, attention_masks_list, sentence_count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "5f2e6dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_attributions(attribution):\n",
    "    attributions = attribution.sum(dim=-1).squeeze(0)\n",
    "    print(torch.norm(attributions))\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d435e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_indexes(filename):\n",
    "    index_df = pd.read_csv(filename, encoding='UTF-8')\n",
    "    index_df.columns = ['Unnamed: 0', 'prediction', 'label', 'score', 'idx']\n",
    "    val_index = sorted(list(index_df.idx.values))\n",
    "    \n",
    "    return val_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7735b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list = get_indexes(f'../predicting-satisfaction-using-graphs/csv/splitbert_classifier/post_comment/seg_seg/epoch_4_result.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "39c6191e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitbert_integrated_gradient_post_comment(index, post, comment, p_sentences, c_sentences, label, score, visualize=False, softmax=False):\n",
    "    \n",
    "    def post_or_comment_or_reply(index):\n",
    "        for i, sentences in enumerate(all_sentences):\n",
    "            if all_tokens[index] in sentences:\n",
    "                if i == 0:\n",
    "                    return 'post'\n",
    "                elif i == 1:\n",
    "                    return 'comment'\n",
    "                else:\n",
    "                    return 'reply'\n",
    "    \n",
    "    input_ids, ref_input_ids, attention_masks, sentence_counts = construct_input_ref_pair([post, comment])\n",
    "    \n",
    "    one_hot_labels = torch.nn.functional.one_hot(torch.tensor(label), num_classes=len(labels))\n",
    "    inputs = {'labels': one_hot_labels.type(torch.float).to(device),\n",
    "          'input_ids1': input_ids[0].to(device),\n",
    "          'input_ids2': input_ids[1].to(device),\n",
    "          'attention_mask1': attention_masks[0].to(device),\n",
    "          'attention_mask2': attention_masks[1].to(device),\n",
    "          'sentence_count1': sentence_counts[0].to(device),\n",
    "          'sentence_count2': sentence_counts[1].to(device)\n",
    "         }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = pc_model(**inputs).hidden_states\n",
    "    inputs = inputs[0]\n",
    "\n",
    "    # inputs = torch.stack(embeddings, dim=0)\n",
    "    baselines = torch.zeros((1, 14, 384))    \n",
    "    pred = forward_func_ig(inputs, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "    base_pred = forward_func_ig(baselines, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "    \n",
    "#     pred = forward_func_ig(inputs, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "#     base_pred = forward_func_ig(baselines, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "#     print(f'pred: {pred}, base_pred: {base_pred}')\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    \n",
    "    i_attribution, i_delta = ig.attribute(inputs=inputs, target=torch.argmax(pred), additional_forward_args=(sentence_counts[0], sentence_counts[1], pc_model), n_steps=50, internal_batch_size=1, return_convergence_delta=True)\n",
    "    b_attribution, b_delta = ig.attribute(inputs=baselines, target=torch.argmax(pred), additional_forward_args=(sentence_counts[0], sentence_counts[1], pc_model), n_steps=50, internal_batch_size=1, return_convergence_delta=True)\n",
    "    print(torch.sum(summarize_attributions(i_attribution)))\n",
    "    print(torch.sum(summarize_attributions(b_attribution)))\n",
    "    attribution, delta = ig.attribute(inputs=inputs, target=torch.argmax(pred), additional_forward_args=(sentence_counts[0], sentence_counts[1], pc_model), n_steps=50, internal_batch_size=1, return_convergence_delta=True)\n",
    "    attributions = summarize_attributions(attribution)\n",
    "    f_attributions = torch.flatten(attributions)\n",
    "    f_attributions = f_attributions[f_attributions.nonzero()].squeeze(1)\n",
    "    abs_attributions = list(map(abs, map(float, f_attributions)))\n",
    "    idx_attributions = []\n",
    "    for j in range(len(abs_attributions)):\n",
    "        idx_attributions.append((j, abs_attributions[j], f_attributions[j].item()))\n",
    "    idx_attributions.sort(key=lambda x:x[1], reverse=True)\n",
    "        \n",
    "    top3 = idx_attributions[:3]\n",
    "    \n",
    "    if visualize:\n",
    "        all_sentences = [['[[post]]'], post, ['[[comment]]'], comment]\n",
    "        all_tokens = [item for all_sentences in all_sentences for item in all_sentences]\n",
    "        \n",
    "        vis_attributions = []\n",
    "        \n",
    "        j = 0\n",
    "        for i in range(len(all_tokens)):\n",
    "            if all_tokens[i] in ['[[post]]', '[[comment]]']:\n",
    "                vis_attributions.append(0)\n",
    "            else:\n",
    "                vis_attributions.append(f_attributions[j].item())\n",
    "                j += 1\n",
    "                \n",
    "        vis_attributions = torch.tensor(vis_attributions)\n",
    "        \n",
    "        score_vis = viz.VisualizationDataRecord(vis_attributions,\n",
    "                                                torch.max(torch.softmax(pred, dim=0)),\n",
    "                                                torch.argmax(pred),  # predicted label\n",
    "                                                f'{label}, {score}',  # true label\n",
    "                                                p_sentences + ' ' + c_sentences,\n",
    "                                                vis_attributions.sum(),\n",
    "                                                all_tokens,\n",
    "                                                delta)\n",
    "        raw_text = ' '.join(post) + ' '.join(comment)\n",
    "        \n",
    "        print('\\033[1m', 'Visualization For Score', '\\033[0m')\n",
    "        viz.visualize_text([score_vis])\n",
    "        print(f'pred: {pred}, base_pred: {base_pred}')\n",
    "        print(f'sub: {pred - base_pred}')\n",
    "        print(f'sum sub: {torch.sum(pred-base_pred)}')\n",
    "        print(vis_attributions)\n",
    "        print('delta: ', delta)\n",
    "        \n",
    "    else:\n",
    "        where = []\n",
    "        \n",
    "        all_sentences = [post, comment]\n",
    "        all_tokens = [item for all_sentences in all_sentences for item in all_sentences]\n",
    "\n",
    "        for j in range(len(top3)):\n",
    "            where.append(post_or_comment_or_reply(top3[j][0]))\n",
    "            result.append([index, post, comment, score, all_tokens[top3[j][0]], top3[j][2], post_or_comment_or_reply(top3[j][0])])\n",
    "\n",
    "        # result.append([index, post, comment, score, all_tokens[top3[0][0]], top3[0][2], where[0]])\n",
    "        # result.append([index, post, comment, score, all_tokens[top3[1][0]], top3[1][2], where[1]])\n",
    "        # result.append([index, post, comment, score, all_tokens[top3[2][0]], top3[2][2], where[2]])\n",
    "\n",
    "        return result, label, torch.argmax(pred).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "46c36516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitbert_integrated_gradient_post_comment(index, post, comment, p_sentences, c_sentences, label, score, visualize=False, softmax=False):\n",
    "    \n",
    "    def post_or_comment_or_reply(index):\n",
    "        for i, sentences in enumerate(all_sentences):\n",
    "            if all_tokens[index] in sentences:\n",
    "                if i == 0:\n",
    "                    return 'post'\n",
    "                elif i == 1:\n",
    "                    return 'comment'\n",
    "                else:\n",
    "                    return 'reply'\n",
    "    \n",
    "    input_ids, ref_input_ids, attention_masks, sentence_counts = construct_input_ref_pair([post, comment])\n",
    "    \n",
    "    one_hot_labels = torch.nn.functional.one_hot(torch.tensor(label), num_classes=len(labels))\n",
    "    inputs = {'labels': one_hot_labels.type(torch.float).to(device),\n",
    "          'input_ids1': input_ids[0].to(device),\n",
    "          'input_ids2': input_ids[1].to(device),\n",
    "          'attention_mask1': attention_masks[0].to(device),\n",
    "          'attention_mask2': attention_masks[1].to(device),\n",
    "          'sentence_count1': sentence_counts[0].to(device),\n",
    "          'sentence_count2': sentence_counts[1].to(device)\n",
    "         }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inputs = pc_model(**inputs).hidden_states\n",
    "    inputs = inputs[0]\n",
    "\n",
    "    # inputs = torch.stack(embeddings, dim=0)\n",
    "    baselines = torch.zeros((1, 14, 384))    \n",
    "    pred = forward_func_ig(inputs, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "    base_pred = forward_func_ig(baselines, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "    \n",
    "#     pred = forward_func_ig(inputs, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "#     base_pred = forward_func_ig(baselines, sentence_counts[0], sentence_counts[1], pc_model)\n",
    "#     print(f'pred: {pred}, base_pred: {base_pred}')\n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for target in [0, 1, 2]:\n",
    "        i_attribution, i_delta = ig.attribute(inputs=inputs, target=target, additional_forward_args=(sentence_counts[0], sentence_counts[1], pc_model), n_steps=50, internal_batch_size=1, return_convergence_delta=True)\n",
    "        b_attribution, b_delta = ig.attribute(inputs=baselines, target=target, additional_forward_args=(sentence_counts[0], sentence_counts[1], pc_model), n_steps=50, internal_batch_size=1, return_convergence_delta=True)\n",
    "        attribution, delta = ig.attribute(inputs=inputs, target=target, additional_forward_args=(sentence_counts[0], sentence_counts[1], pc_model), n_steps=50, internal_batch_size=1, return_convergence_delta=True)\n",
    "        attributions = summarize_attributions(attribution)\n",
    "        f_attributions = torch.flatten(attributions)\n",
    "        f_attributions = f_attributions[f_attributions.nonzero()].squeeze(1)\n",
    "        abs_attributions = list(map(abs, map(float, f_attributions)))\n",
    "        idx_attributions = []\n",
    "        for j in range(len(abs_attributions)):\n",
    "            idx_attributions.append((j, abs_attributions[j], f_attributions[j].item()))\n",
    "        idx_attributions.sort(key=lambda x:x[1], reverse=True)\n",
    "\n",
    "        top3 = idx_attributions[:3]\n",
    "\n",
    "        if visualize:\n",
    "            all_sentences = [['[[post]]'], post, ['[[comment]]'], comment]\n",
    "            all_tokens = [item for all_sentences in all_sentences for item in all_sentences]\n",
    "\n",
    "            vis_attributions = []\n",
    "\n",
    "            j = 0\n",
    "            for i in range(len(all_tokens)):\n",
    "                if all_tokens[i] in ['[[post]]', '[[comment]]']:\n",
    "                    vis_attributions.append(0)\n",
    "                else:\n",
    "                    vis_attributions.append(f_attributions[j].item())\n",
    "                    j += 1\n",
    "\n",
    "            vis_attributions = torch.tensor(vis_attributions)\n",
    "\n",
    "            score_vis = viz.VisualizationDataRecord(vis_attributions,\n",
    "                                                    torch.max(torch.softmax(pred, dim=0)),\n",
    "                                                    torch.argmax(pred),  # predicted label\n",
    "                                                    f'{label}, {score}',  # true label\n",
    "                                                    p_sentences + ' ' + c_sentences,\n",
    "                                                    vis_attributions.sum(),\n",
    "                                                    all_tokens,\n",
    "                                                    delta)\n",
    "            raw_text = ' '.join(post) + ' '.join(comment)\n",
    "\n",
    "            print('\\033[1m', 'Visualization For Score', '\\033[0m')\n",
    "            viz.visualize_text([score_vis])\n",
    "            print(f'target: {target}')\n",
    "            print(f'pred: {pred}, base_pred: {base_pred}')\n",
    "            print(f'sub: {pred - base_pred}')\n",
    "            print(f'sum sub: {torch.sum(pred-base_pred)}')\n",
    "            print(f'sum of input attributions: {torch.sum(summarize_attributions(i_attribution))}')\n",
    "            print(f'baseline attributions: {summarize_attributions(b_attribution)}')\n",
    "            print(vis_attributions)\n",
    "            print('delta: ', delta)\n",
    "\n",
    "        else:\n",
    "            where = []\n",
    "\n",
    "            all_sentences = [post, comment]\n",
    "            all_tokens = [item for all_sentences in all_sentences for item in all_sentences]\n",
    "\n",
    "            for j in range(len(top3)):\n",
    "                where.append(post_or_comment_or_reply(top3[j][0]))\n",
    "                result.append([index, post, comment, score, all_tokens[top3[j][0]], top3[j][2], post_or_comment_or_reply(top3[j][0])])\n",
    "\n",
    "            # result.append([index, post, comment, score, all_tokens[top3[0][0]], top3[0][2], where[0]])\n",
    "            # result.append([index, post, comment, score, all_tokens[top3[1][0]], top3[1][2], where[1]])\n",
    "            # result.append([index, post, comment, score, all_tokens[top3[2][0]], top3[2][2], where[2]])\n",
    "\n",
    "            return result, label, torch.argmax(pred).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a64a29d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device, pc_model, tokenizer = prepare_model('post_comment', '../predicting-satisfaction-using-graphs/splitbert/model/seg_seg/attention', 5, False, 'diagonal', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "691b0f01",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0514, dtype=torch.float64)\n",
      "tensor(0.9504, dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(nan, dtype=torch.float64)\n",
      "tensor(1.0514, dtype=torch.float64)\n",
      "\u001b[1m Visualization For Score \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0, 2.45</b></text></td><td><text style=\"padding-right:2em\"><b>2 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>Im the guy no one suspects is as messed up as I am....Am wealthy, have nice wife, kids, everyone envious of our home (8000 sf so not small in wealthiest county in the USA).....good looking and well you name it ..........But Im now unemployed, wife has and I think still sleeps with other men, am now drinking a lot, no sex in years, kids make jokes about my drinking to wife......I embarass my wife out in piblic, I dont work out anymore and have 50 lbs more than I did a yr and half ago, I am going broke and well Im a loser I think........But when i drink at least the pain and how much of a loser I am doesnt keep me awake all night and makes me feel like I can do stuff when drunk...... As bad as I feel about the depression, and I am pretty sure you are just taking out your pent up anger on him, you came out swinging first by calling him a moron over a little mistsle first</b></text></td><td><text style=\"padding-right:2em\"><b>0.95</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [[post]]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Im the guy no one suspects is as messed up as I am....Am wealthy, have nice wife, kids, everyone envious of our home (8000 sf so not small in wealthiest county in the USA).....good looking and well you name it ..........But Im now unemployed, wife has and I think still sleeps with other men, am now drinking a lot, no sex in years, kids make jokes about my drinking to wife......I embarass my wife out in piblic, I dont work out anymore and have 50 lbs more than I did a yr and half ago, I am going broke and well Im a loser I think........But when i drink at least the pain and how much of a loser I am doesnt keep me awake all night and makes me feel like I can do stuff when drunk......                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [[comment]]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> As bad as I feel about the depression, and I am pretty sure you are just taking out your pent up anger on him, you came out swinging first by calling him a moron over a little mistsle first                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tensor([[1.4614e-03, 1.4889e-05, 9.9852e-01]], grad_fn=<SoftmaxBackward0>), base_pred: tensor([[4.9324e-09, 9.9995e-01, 4.6728e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "sub: tensor([[ 0.0015, -0.9999,  0.9985]], grad_fn=<SubBackward0>)\n",
      "sum sub: 0.0\n",
      "tensor([ 0.0000,  0.9988,  0.0000, -0.0484])\n",
      "delta:  tensor([0.0007], dtype=torch.float64)\n",
      "tensor(1.0516, dtype=torch.float64)\n",
      "tensor(0.9447, dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(nan, dtype=torch.float64)\n",
      "tensor(1.0516, dtype=torch.float64)\n",
      "\u001b[1m Visualization For Score \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>0, 2.45</b></text></td><td><text style=\"padding-right:2em\"><b>0 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>I'm wondering if anyone here has thought about suicide methods that come close to killing you but can't.  I'm not ready to die but I want to hurt.  I want people to see physically how much I'm hurting emotionally.I've thought about jumping in front of cars or falling from non-lethal heights.  Those aren't the nicest methods but I'll go with them if I can't find a better one. Donât do these things please. If other people donât believe how hurt you are emotionally, try your best to let it not affect you.</b></text></td><td><text style=\"padding-right:2em\"><b>0.94</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [[post]]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I'm wondering if anyone here has thought about suicide methods that come close to killing you but can't.  I'm not ready to die but I want to hurt.  I want people to see physically how much I'm hurting emotionally. I've thought about jumping in front of cars or falling from non-lethal heights.  Those aren't the nicest methods but I'll go with them if I can't find a better one.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [[comment]]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Donât do these things please. If other people donât believe how hurt you are emotionally, try your best to let it not affect you.                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tensor([[0.9921, 0.0060, 0.0018]], grad_fn=<SoftmaxBackward0>), base_pred: tensor([[4.9324e-09, 9.9995e-01, 4.6728e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "sub: tensor([[ 0.9921, -0.9939,  0.0018]], grad_fn=<SubBackward0>)\n",
      "sum sub: -5.51808625459671e-08\n",
      "tensor([ 0.0000, -0.0539,  0.0000,  0.9985])\n",
      "delta:  tensor([0.0012], dtype=torch.float64)\n",
      "tensor(0.0029, dtype=torch.float64)\n",
      "tensor(1.5048, dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(nan, dtype=torch.float64)\n",
      "tensor(0.0029, dtype=torch.float64)\n",
      "\u001b[1m Visualization For Score \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1, 4.0</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>I remember being pretty excited for the new sun and moon version and had quite a bit of fun when it came out but now almost a week later I just dont have the energy to finish it, its just not enjoyable anymore. I  feel like depression steals all of the fun in things i liked doing. And the fact that i just failed an exam at uni doesnt help things. Sometimes I dont feel like living but dont want to die either and it sucks, sorry for rambling, just needed to get it out somewhere :c I just started playing a HeartGold ROM on my android device and after 2:37 of playtime I don't see the point anymore. I know how the game plays out already, no point in carrying on for another 70 hours. </b></text></td><td><text style=\"padding-right:2em\"><b>1.50</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [[post]]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 75%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">  I remember being pretty excited for the new sun and moon version and had quite a bit of fun when it came out but now almost a week later I just dont have the energy to finish it, its just not enjoyable anymore. I  feel like depression steals all of the fun in things i liked doing.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 58%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">  And the fact that i just failed an exam at uni doesnt help things. Sometimes I dont feel like living but dont want to die either and it sucks, sorry for rambling, just needed to get it out somewhere :c                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [[comment]]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> I just started playing a HeartGold ROM on my android device and after 2:37 of playtime I don't see the point anymore. I know how the game plays out already, no point in carrying on for another 70 hours.                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tensor([[1.7266e-05, 9.9998e-01, 7.3158e-08]], grad_fn=<SoftmaxBackward0>), base_pred: tensor([[7.2887e-08, 9.9556e-01, 4.4361e-03]], grad_fn=<SoftmaxBackward0>)\n",
      "sub: tensor([[ 1.7193e-05,  4.4187e-03, -4.4360e-03]], grad_fn=<SubBackward0>)\n",
      "sum sub: -5.681067705154419e-08\n",
      "tensor([0.0000, 0.5042, 0.8504, 0.0000, 0.1501])\n",
      "delta:  tensor([1.3400e-05], dtype=torch.float64)\n",
      "tensor(0.9140, dtype=torch.float64)\n",
      "tensor(1.0115, dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(nan, dtype=torch.float64)\n",
      "tensor(0.9140, dtype=torch.float64)\n",
      "\u001b[1m Visualization For Score \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>1, 3.6</b></text></td><td><text style=\"padding-right:2em\"><b>2 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>Probably sounds stupid but the only thing I look forward to everyday is laying in bed after doing nothing all day and just sleeping for 15 hours. I wish I could sleep longer Sertraline (Zoloft) changed my life for real, but everyone is different.</b></text></td><td><text style=\"padding-right:2em\"><b>1.01</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [[post]]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 51%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Probably sounds stupid but the only thing I look forward to everyday is laying in bed after doing nothing all day and just sleeping for 15 hours. I wish I could sleep longer                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [[comment]]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> Sertraline (Zoloft) changed my life for real, but everyone is different.                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tensor([[0.0650, 0.0040, 0.9310]], grad_fn=<SoftmaxBackward0>), base_pred: tensor([[4.9324e-09, 9.9995e-01, 4.6728e-05]], grad_fn=<SoftmaxBackward0>)\n",
      "sub: tensor([[ 0.0650, -0.9960,  0.9309]], grad_fn=<SubBackward0>)\n",
      "sum sub: 5.960464477539063e-08\n",
      "tensor([0.0000, 0.9999, 0.0000, 0.0116])\n",
      "delta:  tensor([-0.0064], dtype=torch.float64)\n",
      "tensor(1.5240, dtype=torch.float64)\n",
      "tensor(0.2112, dtype=torch.float64)\n",
      "tensor(0., dtype=torch.float64)\n",
      "tensor(nan, dtype=torch.float64)\n",
      "tensor(1.5240, dtype=torch.float64)\n",
      "\u001b[1m Visualization For Score \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>2, 6.4</b></text></td><td><text style=\"padding-right:2em\"><b>1 (1.00)</b></text></td><td><text style=\"padding-right:2em\"><b>My girlfriend has been severely depressed for the better part of 2 months and fighting her symptoms in addition to medication adherance and weekly talk therapy just wasn't cutting it.  She made the decision to go inpatient today after a week of intermittent SI and thoughts of SIB.  I work in acute mental health treatment and have chronic mental illness myself and while she's told me that she'll alert me when I've hit a raw spot or need to take off my mental health thinking cap, I've never been on the other side of this, at least not in this deep.  She had a rough patch when I first started dating her, but we weren't living together.  Any recommendations for being supportive of her while she's being hospitalized, with thoughts to how my experience as both a mental health provider and consumer may hinder/help my behavior and thoughts?  I won't be visiting everyday, partially because the hospital is over an hour away and also because she's got support just a phone call away and I know relatively what the environment is going to be like.  I will be visiting a couple of times this week but I'm going to be as unintrusive as I can. Try and be there for her as partner instead of as someone who works in the field. I think it's actually a positive that you are so familiar with how all that jazz works. She's inpatient so she'll have access to doctors, therapists, meds, whatever. What she won't have access is a caring partner, and that's the role you need to fill. Also, be sure and take stock of your own wellness. It can't be easy having an SO struggling as you also have your own afflictions.</b></text></td><td><text style=\"padding-right:2em\"><b>0.21</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [[post]]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">  My girlfriend has been severely depressed for the better part of 2 months and fighting her symptoms in addition to medication adherance and weekly talk therapy just wasn't cutting it.  She made the decision to go inpatient today after a week of intermittent SI and thoughts of SIB.  I work in acute mental health treatment and have chronic mental illness myself and while she's told me that she'll alert me when I've hit a raw spot or need to take off my mental health thinking cap, I've never been on the other side of this, at least not in this deep.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">   She had a rough patch when I first started dating her, but we weren't living together.  Any recommendations for being supportive of her while she's being hospitalized, with thoughts to how my experience as both a mental health provider and consumer may hinder/help my behavior and thoughts?  I won't be visiting everyday, partially because the hospital is over an hour away and also because she's got support just a phone call away and I know relatively what the environment is going to be like.  I will be visiting a couple of times this week but I'm going to be as unintrusive as I can.                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [[comment]]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 57%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">  Try and be there for her as partner instead of as someone who works in the field. I think it's actually a positive that you are so familiar with how all that jazz works.                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\">  She's inpatient so she'll have access to doctors, therapists, meds, whatever. What she won't have access is a caring partner, and that's the role you need to fill. Also, be sure and take stock of your own wellness. It can't be easy having an SO struggling as you also have your own afflictions.                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: tensor([[4.3075e-05, 9.1854e-01, 8.1412e-02]], grad_fn=<SoftmaxBackward0>), base_pred: tensor([[1.0209e-06, 5.9206e-01, 4.0794e-01]], grad_fn=<SoftmaxBackward0>)\n",
      "sub: tensor([[ 4.2054e-05,  3.2649e-01, -3.2653e-01]], grad_fn=<SubBackward0>)\n",
      "sum sub: 2.9802322387695312e-08\n",
      "tensor([ 0.0000, -0.4122, -0.2816,  0.0000,  0.8656,  0.0393])\n",
      "delta:  tensor([-0.0047], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for i in index_list[:5]:\n",
    "    splitbert_integrated_gradient_post_comment(i, post_sequences[i], comment_sequences[i], post_contents[i], comment_bodies[i], satisfactions[i], satisfactions_float[i], True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4ebfe1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ffd910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1213258d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83bbc882",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device, pc_model, tokenizer = prepare_model('post_comment', '../predicting-satisfaction-using-graphs/splitbert/model/seg_seg/wo_softmax', 4, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7d6a4b67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n",
      "torch.Size([384])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a view of a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1460046/2751330555.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindex_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msplitbert_integrated_gradient_post_comment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_sequences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment_bodies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msatisfactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msatisfactions_float\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1460046/1935794033.py\u001b[0m in \u001b[0;36msplitbert_integrated_gradient_post_comment\u001b[0;34m(index, post, comment, p_sentences, c_sentences, label, score, visualize, softmax)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# inputs = torch.stack(embeddings, dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mbaselines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m384\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_func_ig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mbase_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_func_ig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpc_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1460046/1387871891.py\u001b[0m in \u001b[0;36mforward_func_ig\u001b[0;34m(inputs, p_count, c_count, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp_count\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mp_count\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mc_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a view of a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "for i in index_list[:1]:\n",
    "    splitbert_integrated_gradient_post_comment(i, post_sequences[i], comment_sequences[i], post_contents[i], comment_bodies[i], satisfactions[i], satisfactions_float[i], True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc6719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
